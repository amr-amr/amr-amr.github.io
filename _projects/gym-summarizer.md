---
title: "gym-summarizer"
excerpt: "Reinforcement learning environment for extractive summarization"
collection: projects
---
## What is gym-summarizer
[gym-summarizer](https://github.com/amr-amr/gym-summarizer) is an 
[OpenAI Gym](https://gym.openai.com/) environment for extractive summarization, 
a task where the summary of a text is generated by selecting a subset of sentences from it. 
This was a class project for [COMP-767: Reinforcement Learning](https://www.cs.mcgill.ca/~dprecup/courses/rl.html), 
built in collaboration with [Alexander Nicholson](https://github.com/ANich).

In this environment, we define episodes, observations, actions, and rewards as follows:


- __Episodes__ are the generation of an entire summary from a given source text;

- __Observations__ are the concatenated sentence embeddings from the source text 
and from the summary generated so far, truncated or padded with zeros to 
predefined lengths. For example: we use BERT to generate 768-dimensional 
sentence embeddings, define a maximum source text length of 100 sentences, 
and a summary length of 4 sentences; the resulting observation is a 104x768 
dimensional vector.

- __Actions__ are the sentence indices for the source text, up to the predefined 
maximum number of sentences. For example: if an agent outputs actions 0,1,2,99 
in an episode, the first 3 sentences and last sentence are selected for the summary. 
Every time a sentence is selected, its embedding is added to the corresponding 
summary column in the next observation. If an invalid action such as a sentence 
index that has already been selected or that is out-of-bounds for the source text, 
the agent receives a negative reward and the state does not change. 


- __Rewards__ are generated via different configurations of the 
[ROUGE](https://rxnlp.com/how-rouge-works-for-evaluation-of-summarization-tasks/) 
automatic evaluation metric. In particular, we extend ROUGE with 
[potential-based reward shaping](https://www-users.cs.york.ac.uk/~devlin/presentations/pbrs-tut.pdf) 
by measuring the incremental increase of ROUGE with each action. 
This allows dense rewards as opposed to the common sparse reward setting, 
where the ROUGE score isnâ€™t calculated and observed as a reward until the end of the episode. 

# Why we built gym-summarizer
We built gym-summarizer to explore three issues we saw with current approaches 
of applying reinforcement learning to summarization:  


- __Reliance on supervised-learning__:
Most approaches involve supervised pretraining with a maximum 
likelihood objective, followed by fine-tuning with a reinforcement learning 
objective. We wanted to see if effective summarization could be learned solely with RL.

- __Limited baselines for different algorithms__:  
Most approaches use either REINFORCE or actor-critic algorithms. 
We wanted to make it easy to use out-of-the-box models from 
[stable-baselines](https://github.com/hill-a/stable-baselines) 
to e.g. compare value-based methods and policy-based methods.

- __Sparse rewards__:
Most approaches use sparse terminal rewards, 
which leads to a credit assignment problem 
(i.e. how do you know which action was good or bad). 
We wanted to see if dense rewards could improve learning and performance.


# How we built gym-summarizer
## Dataset
We use an existing preprocessed version of the [CNN-DM dataset](https://github.com/JafferWilson/Process-Data-of-CNN-DailyMail).

## Document embeddings
We use [bert-as-service](https://github.com/hanxiao/bert-as-service) and bert-base-uncased to pregenerate sentence embeddings for each document in the dataset.
## Stable baselines algorithms
We use [stable-baselines](https://github.com/hill-a/stable-baselines) to implement and train the PPO2, DQN, and A2C algorithms;


# Experiment Results
## Effect of dense reward;
Using a CNN-LSTM policy and PPO2, we compared the effect of intermediate (dense)
rewards compared to terminal (sparse) rewards. We found that using intermediate rewards
helped with our final performance on a withheld evaluation set.

<br/><img src='/images/projects/gym-summarizer/intermediate.png'>  

<br/><img src='/images/projects/gym-summarizer/terminal.png'>  

## Effect of algorithm;
We experimented with other algorithms (PPO2, DQN, A2C) 
and policy network architectures (MLP, LSTM, CNN-LSTM). 
However, we found no significant change in performance.

## Limitations of end-to-end RL;
We found our RL-generated summaries consistently underperformed a simple Lead-3 baseline
(where a summary is generated by taking the first three sentences).  

<br/><img src='/images/projects/gym-summarizer/lead3.png'>

<br/><img src='/images/projects/gym-summarizer/intermediate.png'>

This is likely due to limitations with our sentence representation which could
not be fine-tuned in our approach.




